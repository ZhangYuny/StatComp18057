---
title: "All_my_homework"
author: "Yunyun Zhang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction to StatComp

__StatComp18057__ is a simple R package which include three functions, namely, _cvm.stat_ (calculate the two-sample Cramer-von Mises statistic value) and _Tn.stat_ (calculate the proportion of first through Kth nearest neighbor coincidences) and _corr_(Simultaneous calculation of correlation coefficients of multiple pairs of vectors).

The source R code for _cvm.stat_ is as follows:
```{r}
cvm.stat<-function(x,y){
  n <- length(x)
  m <- length(y)
  Fn<-ecdf(x)
  Gm<-ecdf(y)
  t<-(m*n/(m+n)^2)*(sum((Fn(x)-Gm(x))^2)+sum((Fn(y)-Gm(y))^2))
  return(t)
}
```

The source R code for _Tn.stat_ is as follows:
```{r}
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]
  n2 <- sizes[2]
  n <- n1 + n2
  z <- z[ix, ]
  o <- rep(0, NROW(z))
  z <- as.data.frame(cbind(z, o))
  NN <- nn2(z,k=k)
  block1 <- NN$nn.idx[1:n1, ]
  block2 <- NN$nn.idx[(n1+1):n, ]
  i1 <- sum(block1 < n1 + .5)
  i2 <- sum(block2 > n1 + .5)
  return((i1 + i2) / (k * n))
}
```

The source R code for _corr_ is as follows:
```{r}
corr<-function(xs,ys){
  c1<-Map(cov,xs,ys)
  v1<-vapply(xs,var,double(1))
  v2<-vapply(ys,var,double(1))
  unlist(Map(function(c,v1,v2) c/((sqrt(v1))*(sqrt(v2))),c1,v1,v2))
}
```



##Homework-2018-09-14
## Question
Write a .Rmd file to implement at least three examples of different types in the above books (texts, numerical results, tables, and figures).

## Answer
## example1
```{r}
x <- rnorm(10)
y <- rnorm(10)
plot(x, y, xlab="Ten random values", ylab="Ten other values", xlim=c(-1, 1), ylim=c(-1, 1), pch=21, col="black", bg="black", bty="l", tcl=0.4, main="How to customize a plot with R", las=1, cex=1.5)
```

## example2
```{r}
data("InsectSprays")
aov.spray <- aov(sqrt(count) ~ spray, data = InsectSprays)
aov.spray
summary(aov.spray)
```

## example3
```{r}
data("women")
fit <- lm(weight ~ height, data=women)
summary(fit)
women$weight
fitted(fit)
residuals(fit)
plot(women$height, women$weight, xlab = "Height (in inches)", ylab = "Weight (in pounds)")
abline(fit)
```


##Homework-2018-09-21
## Question1
A discrete random variable $X$ has probability mass function 

|$x$|0|1|2|3|4|
|-|-|-|-|-|-|
|$p(x)$|0.1|0.2|0.2|0.2|0.3|

Use the inverse transform method to generate a random sample of size 1000 from the distribution of $X$. Construct a relative frequency table and compare the empirical with the theoretical probabilities. Repeat using the R sample function.


##Answer1
First, we use the inverse transform method to generate a random sample from the distribution of $X$. and construct a relative frequency table, then we calculate the ratio between frequency and theoretical probability.Answer

```{r}
x <- c(0,1,2,3,4); p <- c(0.1,0.2,0.2,0.2,0.3)
    cp <- cumsum(p); m <- 1000; r <- numeric(m)
    r1 <- x[findInterval(runif(m),cp)+1]
    ct1 <- as.vector(table(r1));
    f1<-rbind(x,ct1/sum(ct1));rownames(f1)<-c("x","frequency")
    f1
    ct1/sum(ct1)/p

```

Now we use the R sample function to generate a random sample of size 1000 from the distribution of $X$.
```{r}
set.seed(123); n<-1000
r2<-sample(c(0,1,2,3,4),n,replace=T,c(0.1,0.2,0.2,0.2,0.3))
ct2<-as.vector(table(r2))
f2<-rbind(x,ct2/sum(ct2));rownames(f2)<-c("x","frequency")
f2
ct2/sum(ct2)/p
```

## Question2
Write a function to generate a random sample of size n from the $Beta(a, b)$ distribution by the acceptance-rejection method. Generate a random sample of size 1000 from the $Beta(3,2)$ distribution. Graph the histogram of the sample with the theoretical $Beta(3,2)$ density superimposed.

##Answer2
$Beta(3,2)$ with pdf $f(x)=12x^2(1-x),~0<x<1$. Let $c=3$ and $g(x)=x,~0<x<1$.
```{r}
set.seed(123)
n <- 1000;j<-k<-0;y <- numeric(n)
while (k < n) {
u <- runif(1)
j <- j + 1
x <- runif(1) #random variate from g
if (4*x^2*(1-x) > u) {
#we accept x
k <- k + 1
y[k] <- x
}
}
j    # #(experiments) for n random numbers 
```

Quantile-quantile plot for ‘rbeta’ and ‘acceptance-rejection’ algorithm.
```{r}
qqplot(y,rbeta(n,3,2),xlab='Accpetance-rejection',ylab='rbeta')
abline(0,1,col='blue',lwd=2)
```

##Question3
Simulate a continuous Exponential-Gamma mixture. Suppose that the rate parameter $Λ$ has $Gamma(r, β)$ distribution and $Y$ has $Exp(Λ)$ distribution. That is, $(Y |Λ = λ) ～ fY (y|λ) = λe^{-λy}$. Generate 1000 random observations from this mixture with $r = 4$ and $β = 2$.

##Answer3
```{r}
n <- 1000; r <- 4; beta <- 2
lambda <- rgamma(n, r, beta)
y <- rexp(n, lambda)
hist(y,freq=F,main="Histogram of mixture")

y <- seq(1, 20, 0.01)
sp=spline(y,y^2/(sqrt(2*pi))*exp(-y^2/2),n=1000)
sp1=spline(y,(sqrt(2*pi))*exp(-y^2/2),n=1000)
sp2=spline(y,(1/(2*sqrt(2)))*(2/(2+y^2))^{3/2},n=1000)# t-distribution df=2
sp3=spline(y,(2/(sqrt(2)*pi))*(3/(3+y^2))^{2},n=1000)#t-distribution df=3
sp4=spline(y,(3/8)*(4/(4+y^2))^{5/2},n=1000)#t-distribution df=4
plot(sp,type="l",col="red",lwd=2)
lines(sp1,type="l",col="green",lwd=2)
lines(sp2,type="l",col="yellow",lwd=2)
lines(sp3,type="l",col="blue",lwd=2)
lines(sp4,type="l",col="black",lwd=2)
legend("topright",legend=c("g(x)","f1(x)","f2(x)-df=2","f2(x)-df=3","f2(x)-df=4"),col=c("red","green","blue","black"),lwd=2,lty=c(1,1,1,1))
```


##Homework-2018-09-21
## Question1
Write a function to compute a Monte Carlo estimate of the $Beta(3, 3)$ cdf,
and use the function to estimate $F (x)$ for $x = 0.1, 0.2, . . . , 0.9$. Compare the estimates with the values returned by the pbeta function in R.

##Answer1
$Beta(3,3)$ with pdf $f(x)=30x^2(1-x)^2,~0<x<1$,then the $Beta(3, 3)$ cdf 
$F (x)=\int_o^x f(u)du = \int_o^x g(u)\frac1{x}du=E[g(U)].$
where $U\sim U(0,1),g(u)=xf(u).$Use a frequency to approximation the expectation (Strong Law of Large Number): 
$$\frac1m \sum_{i=1}^m g(U_i),$$ 
where $U_1,\ldots,U_m$ are iid copies of $U$ and $m$ is a sufficiently large integer.   
First, using the above principles to Write a function to compute a Monte Carlo estimate of the $Beta(3, 3)$ cdf, and use the function to estimate $F (x)$ for $x = 0.1, 0.2, . . . , 0.9$. then Compare the estimates with the values returned by the pbeta function in R, and the output of the comparison is output in the form of charts.
  
```{r}
MC.func <- function(x, R = 1000) {
  u <- runif(R,0,1)
  cdf <- numeric(length(x))
  for (i in 1:length(x)) {
    g <- 30 * x[i]^3 * u^2 * (1-u*x[i])^2
    cdf[i] <- mean(g) 
  }
  cdf
}
x<-c(seq(0.1,0.9,0.1))
  MC.cdf <- MC.func(x)
  p.cdf <- pbeta(x,3,3)
  f1<-rbind(MC.cdf,p.cdf);rownames(f1)<-c("MC.cdf","p.cdf")
  f1
  plot(x, MC.cdf,type="b",col="red",pch=16,lwd=2,xlab="x",ylab="MC.cdf")
  lines(x, p.cdf,type="b",col="green",pch=16,lwd=2,xlab="x",ylab="MC.cdf")
  legend("topright",legend=c("MC","pbeta"),pch=c(22,16),col=c("red","green"),lwd=2,lty=c(1,2))
```

As can be seen from the graph, the results generated by the written function and the estimates with the values returned by the pbeta function  are very similar,  so the rationality of Monte Carlo method can be obtained.


## Question2
The Rayleigh density [156, (18.76)] is
$$f(x)=\frac{x}{σ^2}e^{-{x^2}/{2σ^2}},~x≥0,σ>0$$
Implement a function to generate samples from a Rayleigh(σ) distribution, using antithetic variables. What is the percent reduction in variance of $\frac{X+X'}{2}$. compared with  for independent $\frac{X_1+X_2}{2}$

##Answer2
   First, We can calculate its distribution function through the density function of Rayleigh distribution:$$F(x)=1-e^{x^2/2σ^2}$$. then generating samples from a Rayleigh($σ$) distribution by the inverse transform method. If X is a continuous random variable with cdf $F(x)$, then $U = F(X) ～ Uniform(0, 1)$, so $F^{-1}_{X}(U)=\sqrt{-2In(1-u)}$ has the same distribution as X.(In this question, we assumed σ=1 )

```{r}
n <- 1000
u <- runif(n)
x <- sqrt(-2*log(1-u))  # F(x) = , 0<=x<=1
hist(x, prob = TRUE, main = expression(f(x)==x*exp(-x^2/2)))
y <- seq(0, 4, .01)
lines(y, y*exp(-y^2/2))
```
   It can be seen from the histogram that the sample distribution produced by the inverse transform method is closer to the theoretical distribution.
   Second, the Monte Carlo estimation of the integral $F(x)$ is implemented in the function MC.FX below. Optionally MC.FX will compute the estimate with or without antithetic sampling, and calculate the theoretical value of the distribution function($F(x)=1-e^{x^2/2σ^2}$) corresponding to each x.

```{r}
MC.FX <- function(x, R = 10000, antithetic = TRUE) {
u <- runif(R/2)
if (!antithetic) v <- runif(R/2) else
v <- 1 - u
u <- c(u, v)
cdf <- numeric(length(x))
for (i in 1:length(x)) {
g <- x[i]^2 *u *exp(-(u * x[i])^2 / 2)
cdf[i] <- mean(g) 
}
cdf
}
x <- seq(0.5, 4, length=8)
set.seed(123)
MC1 <- MC.FX(x, anti = FALSE) #MC1 is the estimate with antithetic sampling
set.seed(123)
MC2 <- MC.FX(x) #MC2 is the estimate without antithetic sampling
FX <-1-exp(-x^2/2)# FX is theoretical distribution value
print(round(rbind(x, MC1, MC2,FX), 5))
```
    From the above data, we can see that the estimate with or without antithetic sampling is very close to the theoretical value. 
    Finally, a comparison of estimates obtained from a single Monte Carlo experiment is below
```{r}
m <- 1000
MC1 <- MC2 <- numeric(m)
x <- 1
for (i in 1:m) {
MC1[i] <- MC.FX(x, R = 1000, anti = FALSE)
MC2[i] <- MC.FX(x, R = 1000)
}
round(c(sd(MC1),sd(MC2),(sd(MC1)-sd(MC2))/sd(MC1)),5)
```
   The antithetic variable approach achieved approximately 69.8% reduction in
variance at x = 1

##Question3
Find two importance functions f1 and f2 that are supported on (1, ∞) and
are ‘close’ to $$g(x)=\frac{x^2}{\sqrt{2π}}e^{-\frac{x^2}{2}},~x>1$$
Which of your two importance functions should produce the smaller variance
in estimating $$\int_1^\infty \frac{x^2}{\sqrt{2π}}e^{-\frac{x^2}{2}} dx $$
by importance sampling? Explain.

##Answer3
we need choose two importance functions to estimate$$\int_1^\infty \frac{x^2}{\sqrt{2π}}e^{-\frac{x^2}{2}} dx $$

by importance sampling method. In this example, $g(x)=\frac{x^2}{\sqrt{2π}}e^{-\frac{x^2}{2}}.$The first candidate for the importance functions are $$f_1(x)= \frac{1}{\sqrt{2π}}e^{-\frac{x^2}{2}} dx $$
This is a density function of normal distribution. we need to plot the densitie on $(0,\infty)$ for comparison with g(x) in Figure1. we find  $f_1(x)$ image is very close to the $g(x)$ image. Considering that the t-distribution is close to the normal distribution, we also draw t-distribution images with different degrees of freedom and compare them with the images of g(x). The density function of t-distribution is as follow.
$$f_2(x)=\frac{\Gamma(\frac{v+1}{2})}{\sqrt{vπ}\Gamma(\frac{v}{2})}(1+\frac{x^2}{v})^{\frac{v+1}{2}}$$
where V is the number of degrees of freedom. We can draw the density function image of the t-distribution of 2,3,4 degrees of freedom.

```{r}
g <-x^2/(sqrt(2*pi))*exp(-x^2)* (x > 1)  #g(x)
f1 <- (sqrt(2*pi))*exp(-x^2/2)* (x > 1)  #f1(x)
f2 <- (1/(2*sqrt(2)))*(2/(2+x^2))^{3/2} * (x > 1) #f2(x) t-distribution df=2
f3 <- (2/(sqrt(3)*pi))*(3/(3+x^2))^{2} * (x > 1) #f3(x) t-distribution df=3
f4 <- (3/8)*(4/(4+x^2))^{5/2} * (x > 1) #f4(x) t-distribution df=4

y <- seq(1, 20, 0.01)
sp=spline(y,y^2/(sqrt(2*pi))*exp(-y^2/2),n=1000)
sp1=spline(y,(sqrt(2*pi))*exp(-y^2/2),n=1000)
sp2=spline(y,(1/(2*sqrt(2)))*(2/(2+y^2))^{3/2},n=1000)# t-distribution df=2
sp3=spline(y,(2/(sqrt(3)*pi))*(3/(3+y^2))^{2},n=1000)#t-distribution df=3
sp4=spline(y,(3/8)*(4/(4+y^2))^{5/2},n=1000)#t-distribution df=4
plot(sp,type="l",col="red",lwd=2)
lines(sp1,type="l",col="green",lwd=2)
lines(sp2,type="l",col="yellow",lwd=2)
lines(sp3,type="l",col="blue",lwd=2)
lines(sp4,type="l",col="black",lwd=2)
legend("topright",legend=c("g(x)","f1(x)","f2(x)-df=2","f2(x)-df=3","f2(x)-df=4"),col=c("red","green","blue","black"),lwd=2,lty=c(1,1,1,1))

```

The densities are plotted on $(0,\infty)$ for comparison with g(x) in Figure 1,
The function that corresponds to the most nearly constant ratio g(x)/f(x)
appears to be f1, From the graphs, we might prefer f1 for the smallest variance.

```{r}
m <- 10000
theta.hat <- se <- numeric(5)
g <- function(x) {
x^2/(sqrt(2*pi))*exp(-x^2) * (x > 1) 
}
 
set.seed(123)
x <- rnorm(m)  #using f1
fg <- g(x) / dnorm(x)
theta.hat[1] <- mean(fg)
se[1] <- sd(fg)

set.seed(123)
x <- rt(m,2)  #using f2
fg <- g(x) / dt(x,2)
theta.hat[2] <- mean(fg)
se[2] <- sd(fg)

set.seed(123)
x <- rt(m,3)  #using f3
fg <- g(x) / dt(x,3)
theta.hat[3] <- mean(fg)
se[3] <- sd(fg)

set.seed(123)
x <- rt(m,4)  #using f4
fg <- g(x) / dt(x,4)
theta.hat[4] <- mean(fg)
se[4]<- sd(fg)

print(c(theta.hat[1],theta.hat[2],theta.hat[3],theta.hat[4]))
print(c(se[1],se[2],se[3],se[4]))
```
so the simulation indicates that f1 produce smallest variance
among these five importance functions, while f2 produces the highest variance.

##Question4
Obtain a Monte Carlo estimate of
$$\frac{x^2}{\sqrt{2π}}e^{-\frac{x^2}{2}}.$$
by importance sampling

##Answer4
According to question3, we can select an important function 
$$f_1(x)= \frac{1}{\sqrt{2π}}e^{-\frac{x^2}{2}} dx $$
If $f_1(x)$ is the importance sampling distribution, and X has pdf $f_1(x)$ supported on A, then
$$\int_1^\infty g(x) dx=\int_1^\infty\frac{g(x)}{f(x)}f(x)=E[\frac{g(X)}{f(X)}] $$
If $X1, . . . , Xn$ is a random sample from the distribution of X, the estimator
is again the sample-mean
$$θ^* =\frac{1}{n}\sum_{k=1}^n\frac{g(X_i)}{f(X_i)}$$

```{r}
m <- 10000
theta.hat <- se <- numeric(5)
g <- function(x) {
x^2/(sqrt(2*pi))*exp(-x^2) * (x > 1) 
}
 
set.seed(123)
x <- rnorm(m)  #using f1
fg <- g(x) / dnorm(x)
theta.hat[1] <- mean(fg)
se[1] <- sd(fg)
print(c(theta.hat[1],se[1]))
```
we can Obtain a Monte Carlo estimate of$θ^*=0.1003274$ 


##Homework-2018-10-12
##Question1
Let X be a non-negative random variable with $μ = E[X] < ∞$. For a random
sample $x1, . . . , xn$ from the distribution of X, the Gini ratio is defined by
$$G=\frac{1}{2n^2\mu}\sum\limits_{j=1}^n\sum\limits_{i=1}^n|x_i-x_j|$$
The Gini ratio is applied in economics to measure inequality in income distribution (see e.g. [163]). Note that G can be written in terms of the order
statistics $x$
(i) as
$$G=\frac{1}{n^2\mu}\sum\limits_{i=1}^n(2i-n-1)x_{(i)}$$
If the mean is unknown, let $\hat{G}$ be the statistic $G$ with $μ$ replaced by $\bar{x} $ Estimate by simulation the mean, median and deciles of $\hat{G}$ if X is standard lognormal. Repeat the procedure for the uniform distribution and Bernoulli(0.1). Also construct density histograms of the replicates in each case.

##Answer1
First, we estimate by simulation the mean, median and deciles of $\hat{G}$ when X is standard lognormal.
```{r}
n <- 100
m <- 1000
Gln <- numeric(m)
set.seed(123)
for (i in 1:m) {
x <- sort(rlnorm(n))
j <- seq(1,n,1)
Gln[i] <- sum((2*j-n-1)*x[j])/ (n^2*exp(0.5))
}
Glnmean<- mean(Gln)
Glnmedian<-median(Gln)
Glndeciles<-quantile(Gln,probs=seq(0,1,0.1))
print(c(Glnmean,Glnmedian,Glndeciles))
hist(Gln, prob = TRUE)
```
The mean, median and deciles are established above. We can see from the above table: the mean is much approaching to median.


Second, we estimate by simulation the mean, median and deciles of $\hat{G}$ when X is uniform distribution.
```{r}
n <- 100
m <- 1000
Gunif <- numeric(m)
set.seed(123)
for (i in 1:m) {
x <- sort(runif(n))
j <- seq(1,n,1)
Gunif[i] <- sum((2*j-n-1)*x[j])/ (n^2*0.5)
}
Gunifmean<- mean(Gunif)
Gunifmedian<-median(Gunif)
Gunifdeciles<-quantile(Gunif,probs=seq(0,1,0.1))
print(c(Gunifmean,Gunifmedian,Gunifdeciles))
hist(Gunif, prob = TRUE)
```

The mean, median and deciles are established above. Mean value is 0.329 which is much approaching to median.

Last, we estimate by simulation the mean, median and deciles of $\hat{G}$ when X is Bernoulli(0.1).
```{r}
n <- 100
m <- 1000
Gbi <- numeric(m)
set.seed(123)
for (i in 1:m) {
x <- sort(rbinom(n,100,0.1))
j <- seq(1,n,1)
Gbi[i] <- sum((2*j-n-1)*x[j])/ (n^2*0.5)
}
Gbimean<- mean(Gbi)
Gbimedian<-median(Gbi)
Gbideciles<-quantile(Gbi,probs=seq(0,1,0.1))
print(c(Gbimean,Gbimedian,Gbideciles))
hist(Gbi, prob = TRUE)
```

The mean, median and deciles are established above. Mean value is 3.3183 which is much approaching to median.



##Qustiion2
Construct an approximate 95% confidence interval for the Gini ratio $\gamma = E[G]$ if $X$ is lognormal with unknown parameters. Assess the coverage rate of the estimation procedure with a Monte Carlo experiment.

##Answer2


By searching for information, we found that $E(G)=2F(\frac{\sigma}{\sqrt{2}})-1$ where $F$ is the distribution function of standard normal. Therefore we can naturally construct an approximate 95% confidence interval for $\gamma=E(G)$ following:
$$(2F(\frac{\sqrt{\frac{(n-1)S^2}{\chi_{n-1}^2(0.025)}}}{\sqrt{2}})-1,2\phi(\frac{\sqrt{\frac{(n-1)S^2}{\chi_{n-1}^2(0.975)}}}{\sqrt{2}})-1).$$ 

```{r}
n<-1000              
m<-100               
l<-100               
mu<-0
sigma<-1
CI.sigma<-function(x,n){
  s2<-(1/(n-1))*sum((x-mean(x))^2)
  c((n-1)*s2/qchisq(0.975,n-1),(n-1)*s2/qchisq(0.025,n-1))
}   
CIG<-cbind(numeric(m),numeric((m)))
Ju<-numeric(m)          
power.G<-numeric(l)
G.ture<-2*pnorm(sigma/sqrt(2))-1 #the true value of G
set.seed(412)
for(j in 1:l){
for(i in 1:m){
  x<-rlnorm(n)
  CIsigma<-CI.sigma(log(x),n)
  CIG[i,]<-2*pnorm(CIsigma/sqrt(2))-1
  Ju[i]<-I(G.ture>CIG[i,1] & G.ture<CIG[i,2])
}
power.G[j]<-mean(Ju)
}
mean(power.G)
```
The coverage rate of estimation is 0.9524 which is a little bigger than 0.95.



##Quenstion 3
Tests for association based on Pearson product moment correlation ρ, Spearman’s rank correlation coefficient $\rho_s$, or Kendall’s coefficient $\tau$, are implemented
in cor.test. Show (empirically) that the nonparametric tests based on $\rho_s$ or $\tau$ are less powerful than the correlation test when the sampled distribution is bivariate normal. Find an example of an alternative (a bivariate
distribution $(X, Y)$ such that $X$ and $Y$ are dependent) such that at least one of the nonparametric tests have better empirical power than the correlation test against this alternative.

##Answer3
Spearman's rank correlation:
```{r}
library("MASS")
n <- 20
m <- 1000
rho.s <- c(seq(-0.8,-0.1,0.1),seq(0.1,0.8,0.1)) 
M <- length(rho.s)
power.s <- numeric(M)
set.seed(123)
for (i in 1:M) {
rho <-rho.s[i]
pvalues.s <- replicate(m, expr = {
x <- mvrnorm(n, c(0,0), matrix(c(1,rho,rho,1),2,2))
cortest.s <- cor.test(x[,1],x[,2],
alternative = "two.sided",method="spearman")
cortest.s$p.value } )
power.s[i] <- mean(pvalues.s<= .05)
}
power.s
```

Kendall's coefficient:
```{r}
n <- 20
m <- 1000
rho.k <- c(seq(-0.8,-0.1,0.1),seq(0.1,0.8,0.1)) #alternatives
M <- length(rho.k)
power.k <- numeric(M)
set.seed(431)
for (i in 1:M) {
rho <-rho.k[i]
pvalues.k <- replicate(m, expr = {
x <- mvrnorm(n, c(0,0), matrix(c(1,rho,rho,1),2,2))
cortest.k <- cor.test(x[,1],x[,2],
alternative = "two.sided",method="kendall")
cortest.k$p.value } )
power.k[i] <- mean(pvalues.k<= .05)
}
power.k
```

Pearson's correlation:
```{r}
n <- 20
m <- 1000
rho.p <- c(seq(-0.8,-0.1,0.1),seq(0.1,0.8,0.1)) #alternatives
M <- length(rho.p)
power.p <- numeric(M)
set.seed(431)
for (i in 1:M) {
rho <-rho.p[i]
pvalues.p <- replicate(m, expr = {
x <- mvrnorm(n, c(0,0), matrix(c(1,rho,rho,1),2,2))
cortest.p <- cor.test(x[,1],x[,2],
alternative = "two.sided",method="pearson")
cortest.p$p.value } )
power.p[i] <- mean(pvalues.p<= .05)
}
power.p
```

```{r}
power.rho<-rbind(power.s,power.k,power.p)
colnames(power.rho)<-c(seq(-0.8,-0.1,0.1),seq(0.1,0.8,0.1))
power.rho
```
As you see, the nonparametric tests based on $ρ_s$ or $τ$ are less powerful than the correlation test when the sampled distribution is bivariate normal.

Pearson's correlation measures linear trend between two datas, it also bases on normality assumption. When the bivariate distribution $(X,Y)$ is not bivariate normal, Pearson's correlation is less significant.

Now, we assume that $X\sim t_{2}$, $Y\sim U(-1,0)$ when $X<0$ and $Y\sim U(0,1)$ when $X\ge0$. Clearly, $X,Y$ are dependent.

```{r}
n <- 20
m <- 1000
set.seed(431)
pvalues.s1 <- replicate(m, expr = {
#simulate under alternative mu1
x <- rt(n,2)
y<-numeric(n)
for(i in 1:n){
  if(x[i]<0) y[i]<-runif(1,-1,0)
  else       y[i]<-runif(1,0,1)
}
cortest.s1 <- cor.test(x,y,
alternative = "two.sided",method="spearman")
cortest.s1$p.value } )
power.s1 <- mean(pvalues.s1<= .05)
power.s1
```

```{r}
n <- 20
m <- 1000
set.seed(431)
pvalues.p1 <- replicate(m, expr = {
#simulate under alternative mu1
x <- rt(n,2)
y<-numeric(n)
for(i in 1:n){
  if(x[i]<0) y[i]<-runif(1,-1,0)
  else       y[i]<-runif(1,0,1)
}
cortest.p1 <- cor.test(x,y,
alternative = "two.sided",method="pearson")
cortest.p1$p.value } )
power.p1 <- mean(pvalues.p1<= .05)
power.p1
```

In this case, the empirical power of the nonparametric test Spearman's rank correlation coefficient $\rho_s$ is 0.987, which is bigger than the empirical power of Pearson correlation $\rho=0.849$.


##Homework-2018-11-2
## Question1
Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

##Answer1
In order to estimate bias and standard error, we need to calculate the statistics of $\hat{\theta}$  from the original samples, then estimating the bias and the standard error by jackknife.
```{r}
#install.packages("bootstrap")
library("bootstrap") #for the law data
theta.hat <-cor(law$LSAT, law$GPA)
print (theta.hat)
n <- nrow(law)    #sample size

#Jackknife estimate of bias of R
theta.jack <- numeric(n)
for (i in 1:n)
theta.jack[i] <- cor(law$LSAT[-i], law$GPA[-i])
bias <- (n - 1) * (mean(theta.jack) - theta.hat)
print(bias) #jackknife estimate of bias  
  
#Jackknife estimate of standard error of R
se <- sqrt((n-1) *
mean((theta.jack - mean(theta.jack))^2))
print(se)

```
According to the results, $\hat{\theta}$=0.776, the bias is -0.0064, the bias is relatively small, and the standard error is 0.1425


## Question2
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures $1/λ$ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ.


##Answer2
Parameters of exponential distribution of $1/λ$ can be estimated by means of sample mean. Sample mean is unbiased estimation of $1/λ$.
```{r}
#install.packages("boot")
library(boot)                     #for boot and boot.ci
data(aircondit, package = "boot")

theta.boot <- function(dat,i) {   #function to compute the statistic
x <- dat
mean(x[i])                        #mean(x) is unbiased estimation of $1/λ$.
}
dat<-aircondit$hours
boot.obj <- boot(dat,statistic = theta.boot, R = 2000)
print(boot.obj)
print(boot.ci(boot.obj,
type = c("basic", "norm", "perc","bca")))
```
From the results, the interval between four methods of estimating confidence interval is different. I think this is related to the different principles of the four methods.
(1)The standard normal bootstrap confidence interval is the simplest approach,
but not necessarily the best, because here we have treated se($\hat{\theta}$) as a known parameter, but in the bootstrap se(se($\hat{\theta}$)) is estimated.
The standard normal confidence interval is neither transformation respecting nor second order accurate.
(2)BCa is a modified version of percentile intervals, BCa corrected “bias ” and adjusted "skewness", so the two methods have different results, BCa have better theoretical properties and better performance in practice.        



## Question3
 Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\hat{\theta}$.

## Answer3
Sample covariance is an unbiased maximum likelihood estimator of $\Sigma$, So the $\hat{\Sigma}$ here is sample covariance. First, we estimate $\hat{\Sigma}$ by jackknife, then calculate leave-one-out estimates of $\hat{\theta}$.( $\theta$ measures the proportion of variance explained by the first principal component.)   
```{r}
data(scor, package = "bootstrap")
n <- nrow(scor)
sigma.hat <- cov(scor)  # Sigma.hat Calculated from original samples
print (sigma.hat)
eig<-eigen(sigma.hat)
eig.value <-eig$values 
theta.hat<-max(eig.value)/sum(eig.value) #theta.hat Calculated from original samples
print(theta.hat)

#compute the jackknife replicates, leave-one-out estimates
theta.jack <- numeric(n)
for (i in 1:n)
theta.jack[i]<-max(cov(scor[-i]))/sum(cov(scor[-i]))

bias <- (n - 1) * (mean(theta.jack) - theta.hat)   #bias
print(bias)

se <- sqrt((n-1) *                                 #standard error
mean((theta.jack - mean(theta.jack))^2))
print(se)

```
According to the results, we Obtain the jackknife estimates of bias and standard
error of $\hat{\theta}$, the bias is -45,701, the standard error is 0.09946.


## Question4
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models.

## Answer4
First, we can draw a general picture of the four fitting curves, then see which method is best by eave-two-out cross validation.
```{r}
#install.packages("DAAG")
library(DAAG); attach(ironslag)
a <- seq(10, 40, .1) #sequence for plotting fits

L1 <- lm(magnetic ~ chemical)
plot(chemical, magnetic, main="Linear", pch=16)
yhat1 <- L1$coef[1] + L1$coef[2] * a
lines(a, yhat1, lwd=2)

L2 <- lm(magnetic ~ chemical + I(chemical^2))
plot(chemical, magnetic, main="Quadratic", pch=16)
yhat2 <- L2$coef[1] + L2$coef[2] * a + L2$coef[3] * a^2
lines(a, yhat2, lwd=2)

L3 <- lm(log(magnetic) ~ chemical)
plot(chemical, magnetic, main="Exponential", pch=16)
logyhat3 <- L3$coef[1] + L3$coef[2] * a
yhat3 <- exp(logyhat3)
lines(a, yhat3, lwd=2)

L4 <- lm(log(magnetic) ~ log(chemical))
plot(log(chemical), log(magnetic), main="Log-Log", pch=16)
logyhat4 <- L4$coef[1] + L4$coef[2] * log(a)
lines(log(a), logyhat4, lwd=2)


```


```{r}
library(bootstrap) 
library(DAAG); data(ironslag)
n <- length(magnetic) #in DAAG ironslag
shrinkage <- function(fit,k=n/2){
require(bootstrap)
theta.fit <- function(x,y){lsfit(x,y)}
theta.predict <- function(fit,x){cbind(1,x)%*%fit$coef}
x <- fit$model[,2:ncol(fit$model)]
y <- fit$model[,1]
results <- crossval(x,y,theta.fit,theta.predict,ngroup = k)#crossval函数实现交叉验证
r2 <- cor(y,fit$fitted.values)^2
r2cv <- cor(y,results$cv.fit)^2
cat("Original R-square =",r2,"\n")
cat("leave-two-out Cross-Validated R-square =",r2cv,"\n")
cat("Change =",r2-r2cv,"\n")
}


#install.packages("DAAG")
ironslag <- as.data.frame(ironslag[,c("magnetic","chemical")])
fit1 <- lm(magnetic~chemical,data=ironslag)  #model1:Linear
shrinkage(fit1)

fit2 <- lm(magnetic ~ chemical + I(chemical^2)) #model2:Quadratic
shrinkage(fit2)

fit3 <- lm(log(magnetic) ~ chemical) #model3:Exponential
shrinkage(fit3)

fit4 <- lm(log(magnetic) ~ log(chemical))  #model4:Log-Log
shrinkage(fit4)



```

According to the prediction error criterion, Model four, the Log-Log model,
would be the best fit for the data.

##Homework-2018-11-16
##Question1-Exercises8.1
Implement the two-sample Cram′ er-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.


##Answer
First,we can define a function  by the formula of Cramer-von Mises statistics, then test for equal distributions by permutation distribution of the statistic.
```{r}
#A function is defined to calculate the Cramer-von Mises statistics of two sample univariate data.
W2<-function(x,y,n,m){
n <- length(x)
m <- length(y)
Fn<-ecdf(x)
Gm<-ecdf(y)
t<-(m*n/(m+n)^2)*(sum((Fn(x)-Gm(x))^2)+sum((Fn(y)-Gm(y))^2))
return(t)
}     

#Retrieving raw data
attach(chickwts)  #data
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
detach(chickwts)

#Setting function parameters
R <- 999     #number of replicates
z <- c(x, y) #pooled sample
K <- 1:26    #sizes of  pooled sampl
W <- numeric(R) #storage for replicates 

W20<-W2(x,y) # Cramer-von Mises statistics calculated from original samples

for (i in 1:R) {
##generate indices k for the first sample
k <- sample(K, size = 14, replace = FALSE)
x1 <- z[k]
y1 <- z[-k] #complement of x1
W[i] <- W2(x1,y1)
} # Cramer-von Mises statistics calculated from permutation samples

p <- mean(c(W20, W) >= W20) #p-vaule

print(p)
```
As a result, the approximate p-value 0.414 does not support the alternative hypothesis that distributions differ.

##Question3-Exercises9.3
Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchy or qt with df=1). Recall that a Cauchy(θ,η) distribution has density function
$$\ f(x)=\frac{1}{\theta\pi (1 + [(x - \eta)/\theta]^2)} $$ $-\infty <x<\infty ,  \theta > 0$
The standard Cauchy has the Cauchy($\theta = 1,\eta = 0$) density. (Note that the standard Cauchy density is equal to the Student t density with one degree of freedom.)


##Answer

```{r}
#generating chain
m <- 10000
x <- numeric(m)
x[1] <- rnorm(1)
k <- 0
u <- runif(m)
for (i in 2:m) {
xt <- x[i-1]
y <- rnorm(1,xt,1)
num <- dt(y, 1) * dnorm(xt,y,1)
den <- dt(xt, 1) * dnorm(y,xt,1)
if (u[i] <= num/den) x[i] <- y else {
x[i] <- xt
k <- k+1 #y is rejected
}
}

#compare the deciles of the generated observations with the deciles of the standard Cauchy distribution 
b <- 2001 #discard the burnin sample
y <- x[b:m]
a <- seq(0.1,0.9,0.1)
QR <- qt(a,1)  #quantiles of the standard Cauchy
Q <- quantile(x, a)
print(rbind(QR,Q))

#compares the quantiles of the target standard Cauchy distribution with the quantiles of the generated chain in a quantile-quantile plot (QQ plot).
b<-ppoints(100)
QR1 <- qt(b,1) #quantiles of the standard Cauchy
Q1 <- quantile(x, b)
par(mfrow=c(1,2))
qqplot(QR1, Q1, main="",xlab="standard Cauchy Quantiles", ylab="Sample Quantiles")
hist(y, breaks="scott", main="", xlab="", freq=FALSE)
lines(QR1, dt(QR1,1))
```
The result shows that the rejection rate is about 20%. There are some gaps between the deciles of the generated observations and the deciles of the standard Cauchy distribution, but they are generally close, this can also be seen in the following two figures


##Question3-Exercises9.6
Rao [220, Sec. 5g] presented an example on genetic linkage of 197 animals in four categories (also discussed in [67, 106, 171, 266]). The group sizes are (125,18,20,34). Assume that the probabilities of the corresponding multinomial distribution are
$$(\frac{1}{2} + \frac{\theta}{4},\frac{1-\theta}{4},\frac{1-\theta}{4},\frac{\theta}{4})$$
Estimate the posterior distribution of θ given the observed sample, using one of the methods in this chapter.

##Answer


```{r}

prob <- function(y, gsize) {
# computes (without the constant) the target density
if (y < 0 || y >= 1)
return (0)
return((1/2+y/4)^gsize[1] *
((1-y)/4)^gsize[2] * ((1-y)/4)^gsize[3] *
(y/4)^gsize[4])
}

gsize<-c(125,18,20,34)
w <- .5 #width of the uniform support set
m <- 5000 #length of the chain
burn <- 1000 #burn-in time
animals <- 197
x <- numeric(m) #the chain

u <- runif(m) #for accept/reject step
v <- runif(m, -w, w) #proposal distribution
x[1] <- .5
for (i in 2:m) {
y <- x[i-1] + v[i]
if (u[i] <= prob(y, gsize) / prob(x[i-1], gsize))
x[i] <- y else
x[i] <- x[i-1]
}

xb <- x[(burn+1):m]
print(round(mean(xb), 3))
```
The sample mean of the generated chain is 0.625 ,this is the estimate of \theta


##Homework-2018-11-30
##Question
Write a function to compute the cdf of the Cauchy distribution, which has
density$$\frac{1}{\theta\pi(1+[(x-\eta)/\theta]^2)},  -\infty<x<\infty $$
where $\theta$ > 0. Compare your results to the results from the R function pcauchy.
(Also see the source code in pcauchy.c.)

##Answer
```{r}

x<-c(-100:100)
f1<-as.numeric(length(x))
f2<-as.numeric(length(x))
f3<-as.numeric(length(x))
f4<-as.numeric(length(x))

for(i in 1:length(x)){
g <- function(y,theta,eta) {
1/(theta*pi*(1+((y-eta)/theta)^2))
}
 
#eta=0,theta=1,standard,Standard Cauchy distribution 
f1[i]<-integrate(g, lower=-Inf, upper=x[i],
rel.tol=.Machine$double.eps^0.25,
eta=0,theta=1)$value
#eta=-10,theta=1
f2[i]<-integrate(g, lower=-Inf, upper=x[i],
rel.tol=.Machine$double.eps^0.25,
eta=-10,theta=1)$value
#eta=10,theta=1
f3[i]<-integrate(g, lower=-Inf, upper=x[i],
rel.tol=.Machine$double.eps^0.25,
eta=10,theta=1)$value
#eta=0,theta=10
f4[i]<-integrate(g, lower=-Inf, upper=x[i],
rel.tol=.Machine$double.eps^0.25,
eta=0,theta=10)$value
}


```


##Question2
A-B-O blood type problem
Let the three alleles be A, B, and O.
|Genotype|AA|BB|00|AO|BO|AB| |
$$|-|-|-|-|-|-|-|-|
|Frequency|p2|q2|r2|2pr|2qr|2pq|1|
|-|-|-|-|-|-|-|-|
|Count|nAA|nBB|n00|nAO|nBO|nAB|n|$$
1.Observed data: nA· = nAA + nAO = 28 (A-type),nB· = nBB + nBO = 24 (B-type), nOO = 41 (O-type), nAB = 70(AB-type).
2.Use EM algorithm to solve MLE of p and q (consider missing data nAA and nBB).
3.Record the maximum likelihood values in M-steps, are they increasing


##Ansewer
```{r}
p0<-0.2; q0<-0.2   #initial estimates
nA<-28; nB<-24; nOO<-41; nAB<-70  #observed data
tol <- .Machine$double.eps^0.5
N <- 10000 #max. number of iterations
p<-p0; q<-q0
El<-numeric(N) #Record the maximum likelihood values in M-steps
k<-1  #Record the circle times
for (i in 1:N) {
a<-p/(2-p-2*q)
b<-q/(2-2*p-q)
Hp=nA*(1+a)+nAB
Hq=nB*(1+b)+nAB
Hpq=nA*(1-a)+nB*(1-b)+2*nOO

El[i]<-Hp*log(p)+Hpq*log(1-p-q)+Hq*log(q) #Record the maximum likelihood values in M-steps

x<-p; y<-q                #storage previous estimates
p<-Hp/(Hp+Hq+Hpq); q<-Hq/(Hp+Hq+Hpq)
k<-k+1
if (abs(p-x)/x<tol && abs(q-y)/y<tol) break
p.hat<-p; q.hat<-q
El<-El[1:k]
}
print(c(p.hat,q.hat))
plot(c(1:length(El)),El,type = "b")

```

From the results, we can see that the number of iterations is 10, and the value of the maximum likelihood values in M-steps is not always increasing, the $\hat{p}$ and $\hat{q}$ obtained by EM algorithm are 0.3273442 and 0.3104267.


##Homework-2018-12-7
##Question1-11.1.2 Exercises3
Use both for loops and lapply() to fit linear models to the
mtcars using the formulas stored in this list:
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

##Answer
```{r}
attach(mtcars)
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

#with a for loop
lm.medel<-vector("list",length(formulas))
for(i in seq_along(formulas)){
lm.medel[[i]]<-lm(formula=formulas[[i]])}
lm.medel

#with lapply
lapply(formulas, function(x) lm(x))
```



##Question1-11.1.2 Exercises4
Fit the model mpg ~ disp to each of the bootstrap replicates
of mtcars in the list below by using a for loop and lapply().
Can you do it without an anonymous function?
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})

##Answer
```{r}
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]#一个bootstrap样本
})

lapply(bootstraps,function(x)lm(mpg ~ disp,data=x))

```

I can't do it without an anonymous function, because the pieces of x are always supplied as the first argument to f, but x here is not the first argument of f.  


##Question1-11.1.2 Exercises5
For each model in the previous two exercises, extract R2 using
the function below.
rsq <- function(mod) summary(mod)$r.squared

##Answer
```{r}
data(mtcars)
#for exercises4
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

mod1<-vector("list",length(formulas))
mod1<-lapply(formulas, function(x) lm(x)) #mod类型也是一个list
unlist(lapply(mod1, function(x) summary(x)$r.squared))

#for exercises5
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]#一个bootstrap样本
})

mod2<-vector("list",length(bootstraps))
mod2<-lapply(bootstraps,function(x) lm(mpg ~ disp,data=x))
unlist(lapply(mod2, function(x) summary(x)$r.squared))

```


##Question1-11.2.5 Exercises3
3. The following code simulates the performance of a t-test for
non-normal data. Use sapply() and an anonymous function
to extract the p-value from every trial.
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
Extra challenge: get rid of the anonymous function by using
[[ directly.

##Answer
```{r}
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)

#with sapply
sapply(trials,function(x) x$p.value)


```

##Question1-11.2.5 Exercises6
6. Implement a combination of Map() and vapply() to create an
lapply() variant that iterates in parallel over all of its inputs
and stores its outputs in a vector (or a matrix). What arguments should the function take?
```{r}
corr<-function(xs,ys){
c1<-Map(cov,xs,ys)
v1<-vapply(xs,var,double(1))
v2<-vapply(ys,var,double(1))
unlist(Map(function(c,v1,v2) c/(v1*v2),c1,v1,v2))
}

x <- replicate(5, runif(10), simplify = FALSE)
y <- replicate(5, rnorm(10), simplify = FALSE)
corr(x,y)
```

##Homework-2018-12-14
##Question1
4. Make a faster version of chisq.test() that only computes the
chi-square test statistic when the input is two numeric vectors
with no missing values. You can try simplifying chisq.test()
or by coding from the mathematical definition (http://en.
wikipedia.org/wiki/Pearson%27s_chi-squared_test).

##Answer1
```{r}
#install.packages("microbenchmark") 
library(microbenchmark)

chisq.test1<-function (x) 
{
        n <- sum(x)
        METHOD <- "Pearson's Chi-squared test"
        nr <- as.integer(nrow(x))
        nc <- as.integer(ncol(x))
        sr <- rowSums(x)
        sc <- colSums(x)
        E <- outer(sr, sc, "*")/n
        v <- function(r, c, n) c * r * (n - r) * (n - c)/n^3
        V <- outer(sr, sc, v, n)
        dimnames(E) <- dimnames(x)
       
       
        STATISTIC <- sum((x - E)^2/E)
        names(E) <- names(x)
       
    names(STATISTIC) <- "X-squared"
   
    
    structure(list(statistic = STATISTIC))
}
set.seed(123)
x<- matrix(sample(1:100,size=20),ncol=2)
suppressWarnings(t1<-chisq.test1(x))
t1  
suppressWarnings(t2<-chisq.test(x)$statistic)
t2 
suppressWarnings(microbenchmark(
t1 = chisq.test1(x),
t2 = chisq.test(x)
))


```
The results show that the calculation time is reduced by 2 times.


##Question2
5. Can you make a faster version of table() for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?

##Answer2
```{r}
table1<-function (x) 
{y <- array(tabulate(x))
class(y) <- "table"
    y
}


Y<-1:5
X<-sample(Y,size=100,replace = TRUE)
table1(X) 
table(X) 

microbenchmark(
t1 = table1(x),
t2 = table(x)
)


```
From the results, we can see that the running speed has been improved a lot.


